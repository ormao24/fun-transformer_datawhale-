
- [ ] Attention 机制
没有Attention模型前的神经机器翻译，是基于Encoder - Decoder 的 RNN / LSTM的。编码器和解码器都是 LSTM/RNN 单元的堆叠。
工作原理：
1. 编码器 LSTM 用于处理整个输入句子并将其编码为上下文向量，这是 LSTM/RNN 的最后一个隐藏状态。这应该是输入句子的一个很好的总结。编码器的所有中间状态都被忽略，最终状态 id 被认为是解码器的初始隐藏状态。
2. 解码器 LSTM 或 RNN 单元依次生成句子中的单词。

存在的主要问题
•	固定长度上下文瓶颈：输入句子较长时，信息压缩导致翻译质量下降。
•	长距离依赖问题：RNN 处理长序列时，早期信息可能丢失。
•	词对齐能力不足：无法动态关注源语言的不同部分，影响翻译准确性。

Attention 机制的核心就是“加权求和”。它通过计算 查询（Query）、键（Key）和值（Value） 之间的关系，来分配不同部分的信息权重，从而更有效地进行信息聚合。￼
其中：
•	Query (Q)：查询向量，代表当前任务需要查找的信息。
•	Key (K)：键向量，代表数据集中不同元素的特征。
•	Value (V)：值向量，表示最终输出的信息。

•	Key 的设计决定了 Query 能否找到正确的 Value，因此 Key 至关重要。
•	Key 需要能够正确表达信息的特征，否则注意力可能会分配错误，影响模型的性能。

举例 1：课堂听讲（注意力集中在关键信息上）

假设你在听一位老师讲课，老师讲了一整节课的信息量非常大，但你不可能记住每一句话。相反，你会根据 问题（Query），寻找相关的 知识点（Key），然后只记住最重要的 信息（Value）。
•	Query（Q）：考试题目
•	Key（K）：老师讲过的知识点
•	Value（V）：对应的详细解释
•	计算方式：你会对不同的知识点赋予不同的注意力权重（重要的知识点记得清楚，不重要的可能直接忽略）。
•	结果：你记住的考试重点，就是 Attention 机制最终的输出。

- [ ] Self-Attention自注意力机制

能够动态地为输入序列中的每个词分配权重，从而捕捉 远距离依赖关系 和 语义关联。

在传统 RNN/CNN 结构中：
•	RNN 只能逐步处理数据，导致 长距离依赖难以捕捉。
•	CNN 主要通过固定窗口的卷积核处理局部信息，无法建模全局依赖。
Self-Attention 解决了这些问题，使得每个词都可以与整个句子中的所有词交互，并且根据语境自动调整注意力分布。
￼

 多头注意力（Multi-Head Attention）增强理解能力
Self-Attention 还可以使用 Multi-Head Attention，即多个注意力头（多个 Query-Key-Value 计算通道）并行地关注不同的信息，进一步提升模型的理解能力。

假设你在评价一部电影：
•	Head 1 关注剧情是否吸引人。
•	Head 2 关注演员的表现。
•	Head 3 关注音乐和特效。

多头注意力使得模型可以同时考虑多个角度，从而理解更复杂的语义。

Self-Attention 通过 动态调整词与词之间的关系，使模型能够：
1.	捕捉长距离依赖（远程指代、翻译上下文）。
2.	消除歧义（解析复杂句子结构）。
3.	理解多义词（根据上下文调整词义）。
4.	多角度关注信息（通过 Multi-Head Attention 提高理解能力）。

- [ ] 词嵌入（Word Embedding）
词嵌入通常指的是通过某种方法（如Word2Vec、GloVe、FastText等）将单词映射到一个固定维度的向量空间的过程。这些方法通过模型学习单词与单词之间的关系、上下文或语义信息，生成的词嵌入向量反映了这些语义或上下文特征。
•	词嵌入（Word Embedding）是整个过程，它将单词从一个高维、稀疏的表示（如one-hot编码）转换为低维、密集的向量。
•	词向量（Word Vector）是词嵌入过程的结果，即表示单词的实际向量。

在神经网络中，nn.Embedding层用于将离散的单词（或其他离散符号）映射为连续的低维向量。nn.Embedding层有一个权重矩阵，该矩阵在训练过程中会被优化。这个权重矩阵通常有两个常见的初始化选择：

1. 随机初始化（Random Initialization）

这种方法将nn.Embedding层的权重矩阵初始化为一个随机的值。常见的做法是使用均匀分布或正态分布来随机初始化这些权重。
•	均匀分布：每个词向量从一个给定范围内的均匀分布中随机生成（例如[-0.1, 0.1]）。
•	正态分布：每个词向量从一个标准正态分布中随机生成。

这种初始化方法的好处是简单且适用于大多数情况，但需要通过反向传播来学习合理的词向量表示。

2. 预训练初始化（Pre-trained Initialization）

另一种常见的初始化方法是使用已经训练好的词向量（如Word2Vec、GloVe、FastText等）来初始化nn.Embedding层的权重矩阵。
•	预训练词向量已经通过大规模语料库训练得到，并能够捕捉到丰富的语义信息。
•	使用预训练的词向量进行初始化可以帮助模型更快地收敛，并在某些任务中提高性能，尤其是当数据量有限时。

这种方法的好处是利用已有的语义信息，不需要从头开始训练词向量。然而，缺点是如果预训练词向量与任务不完全匹配，可能会影响模型的表现。

*"Trainable"与"Freeze"（冻结）相对：

当一个参数被标记为"Trainable"时，意味着它将参与训练过程，即它的值会随着训练的进行而改变。相反，如果一个参数被"Freeze"（冻结），则其值在训练过程中保持不变，即使计算出了梯度也不会更新这个参数。

- [ ] 词向量（Word Vector）

词向量（Word EVector） 是用固定维度的数值向量来表示单词，使得机器可以理解文本中的语义和关系。
在传统 NLP 方法中，单词通常被视为离散的符号（如 “apple” 和 “banana”），无法直接计算它们之间的关系。而词向量的核心思想是将单词映射到一个高维空间，使得语义相近的单词具有相似的向量表示。

词向量的优势在于：
✅ 可以用低维向量（如 100 维）表示单词，减少存储和计算成本。
✅ 能够表示单词的语义关系，使得“相似单词的向量相近”。
✅ 可以用于计算单词之间的数学关系（如 “king - man + woman = queen”）。

目前主流的词向量模型主要有：
1.	Word2Vec（CBOW & Skip-gram）
•	训练方式：让单词预测周围词（CBOW）或让周围词预测单词（Skip-gram）。
•	语义关系：可以计算 “king - man + woman ≈ queen” 这种关系。
2.	GloVe（Global Vectors for Word Representation）
•	训练方式：基于共现矩阵，统计单词共现频率来学习词向量。
•	优势：可以保留全局语义信息，适用于大规模文本。
3.	FastText
•	训练方式：基于 Word2Vec，但引入字符 n-gram，可以处理未登录词（OOV）。
•	适用于拼写变化多的语言（如德语、法语）。
4.	BERT / Transformer-based Embeddings
•	现代 NLP 任务中，使用 BERT、GPT 这样的预训练语言模型来生成动态上下文相关的词向量。
•	例如，“bank” 在 “river bank” 和 “bank account” 中的向量不同，能更好地理解上下文。

从数学角度看，词向量训练的本质就是优化一个损失函数，使得语义相似的单词在向量空间中靠得更近。
无论是 GloVe 还是 Word2Vec，它们的目标都是通过最大化概率或最小化误差来学习最佳词向量。

- [ ] 位置编码(Position Embedding)
位置编码（Position Encoding）是在Transformer模型中引入的一种机制，用于为输入的序列数据（如文本中的词语）添加位置信息。
由于Transformer模型并不像RNN或LSTM那样通过序列的顺序进行信息处理，它是基于并行计算的，并不天然考虑词语在序列中的位置关系。因此，需要显式地将位置信息编码到输入中，来帮助模型理解序列的顺序。
￼
位置编码的实现方式

Transformer中的位置编码通常有两种常见的方式：

(1) 正弦和余弦位置编码（Sinusoidal Position Encoding）

这种位置编码是通过正弦和余弦函数来生成的，这种位置编码的好处是，它是可区分的，并且可以有效地表示长距离词汇之间的相对位置关系。由于正弦和余弦的周期性，模型能够通过不同的频率的正弦和余弦波来区分不同位置的词。

(2) 学习型位置编码（Learned Position Encoding）

这种方法与词嵌入类似，位置编码是通过训练得到的，而不是依赖于正弦和余弦函数。在这种方法中，模型在训练过程中学习到每个位置的编码表示。

这种方法的优点是更加灵活，能够根据数据的特点来调整位置编码。但是它也可能在某些情况下不如正弦和余弦位置编码那样具备强的泛化能力。

3. 位置编码的使用

在Transformer模型中，位置编码会被加到词嵌入向量中。通常，词嵌入向量和位置编码是逐元素相加，得到的结果会被送入后续的网络层（如多头自注意力层）。

4. 为什么位置编码重要
•	捕获顺序信息：位置编码的核心目的是告诉模型每个词在句子中的相对位置，而Transformer本身并不具备这种顺序信息。
•	并行计算：由于Transformer的自注意力机制使得模型能够并行计算输入序列中的所有词，因此它并不像RNN那样按顺序处理数据。位置编码提供了顺序信息，使得并行计算不会丢失序列的顺序关系。
