- [ ] Seq2Seq模型（Sequence to Sequence模型）

是一种用于处理序列数据的深度学习模型，特别适用于需要将一个序列转换为另一个序列的任务，比如机器翻译、文本摘要、语音识别等。它通常由编码器（Encoder）和解码器（Decoder） 两部分组成，核心技术包括循环神经网络（RNN）、长短时记忆网络（LSTM）、门控循环单元（GRU），以及**注意力机制（Attention）**的增强。

Seq2Seq 模型结构
1.	编码器（Encoder）
•	负责接收输入序列，并将其压缩成一个固定长度的“上下文向量”（context vector）。
•	常见的编码器结构是 RNN/LSTM/GRU，逐步处理输入序列的每个时间步，并更新隐藏状态（hidden state）。
•	例如，在机器翻译任务中，编码器会读取英语句子（输入），并将其转换为一个表示整个句子的隐藏状态。
2.	解码器（Decoder）
•	以编码器的“上下文向量”为初始状态，逐步生成目标序列（输出）。
•	通过 RNN/LSTM/GRU 结构，解码器会根据前一个时间步的输出和当前隐藏状态，预测当前时间步的输出单词（或字符）。
•	例如，在机器翻译任务中，解码器会根据编码器的输出，逐步生成法语句子。
3.	注意力机制（Attention，可选但常用）
•	传统 Seq2Seq 的瓶颈问题：编码器需要把整个输入句子压缩成一个固定大小的向量，导致长序列信息可能丢失。
•	注意力机制 允许解码器在生成每个单词时，动态地关注输入序列的不同部分，提高翻译或摘要的效果。

￼￼
在 Seq2Seq 框架提出之前，深度神经网络在图像分类等问题上取得了非常好的效果。在其擅长解决的问题中，输入和输出通常都可以表示为固定长度的向量，如果长度稍有变化，会使用填充（Padding）等操作，其目的是将序列数据转换成固定长度的格式，以便于输入到需要固定长度输入的神经网络中。
举个例子
假设我们有一个包含单词索引的句子序列，目标长度为5：

原始序列：[1, 2, 3]（长度为3）

Padding后的序列：[1, 2, 3, 0, 0]（长度为5，其中0是Padding的元素）


-Padding可能引入的问题

例如在计算损失时可能需要特殊的处理来忽略这些补零的元素，以免它们影响模型的训练效果。这通常通过使用掩码（Mask）来实现，掩码可以指示哪些位置是补零的，以便在计算过程中忽略这些位置。

✅ Seq2Seq 的优点

1. 一站式学习体验：直接从原始的文本或语音等数据中学习，然后直接输出我们想要的结果，整个过程不需要人工去提取特征或进行复杂的预处理。 
2. 灵活应对不同长度 
3. 高效的信息提炼：把一大段文字浓缩成一个简短的摘要。 
4. 易于扩展和升级：可以轻松地与其他神经网络技术（比如卷积神经网络或循环神经网络）结合。

❌ Seq2Seq 的缺点

1. 信息打包难题 想象一下，你试图把一本厚厚的百科全书的内容全部塞进一个小小的记忆卡片里。Seq2Seq模型在做类似的事情，它需要把整个输入序列的信息压缩成一个固定大小的上下文向量。这就好比你只能记住百科全书的概要，而丢失了很多细致入微的细节。
2. 记忆挑战 Seq2Seq模型有时候就像一个有短期记忆障碍的人，它很难回忆起很久以前发生的事情。这意味着，当处理很长的序列时，模型往往难以捕捉到序列开始和结束之间的长期依赖关系，就像试图回忆一个长故事的每一个细节一样困难。
3. 训练与实战的差距 在训练过程中，我们常常用一种叫做“teacher forcing”的方法来训练Seq2Seq模型，这就像是考试时老师不断给你提示答案。但是，在实际使用模型时，它需要自己猜测下一步该做什么，这就可能导致模型在实际应用时表现不如训练时那么出色，因为它可能会过分依赖那些训练时的“提示”，而忽略了如何独立解决问题。这种训练和实际应用之间的差异，我们称之为“Exposure Bias”。


- [ ] Attention 解决信息丢失问题

Attention 模型的特点是 Encoder 不再将整个输入序列编码为固定长度的“向量C” ，而是编码成一个向量（Context vector）的序列（“C1”、“C2”、“C3”），解决“信息过长，信息丢失”的问题。

Attention 的核心工作就是“关注重点”。在特定场景下，解决特定问题。
* 场景：信息量大，包含有效信息和噪声。
* 问题：大脑算力和资源有限，无法同时处理所有信息。
* 应对：快速从繁杂信息中检索出对解决问题最重要的信息。


- [ ] 全局注意力和局部注意力

￼
当应用“全局”注意力层时，会产生大量计算。这是因为必须考虑所有隐藏状态，将它们连接成一个矩阵，并与正确维度的权重矩阵相乘，才能得到前馈连接的最后一层。
软注意力是全局注意力，其中所有图像块都被赋予一定的权重；但在硬注意力中，一次只考虑一个图像块。

Transformer模型
￼
￼transformer模型本质上是一个Encoder-Decoder的结构。输入序列(inputs)先进行Embedding，经过Encoder之后结合上一次output再输入Decoder，最后用softmax计算序列下一个word的概率。

- [ ] Softmax

Softmax 是一种常用于 多分类任务 的激活函数，它将一组 实数输入 转换为 概率分布，使得所有输出值的和为 1。Softmax 主要用于 神经网络的输出层，特别是在分类问题（如图像分类、文本分类）中。

Softmax 公式
假设有一个向量 \mathbf{z} = (z_1, z_2, …, z_n)，Softmax 计算方式如下：


\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}


其中：
	•	 z_i  是输入的某个分数（logit）。
	•	 e^{z_i}  代表指数运算，确保所有输出值都是正数。
	•	归一化项  \sum_{j=1}^{n} e^{z_j}  确保所有输出的总和为 1。
￼
￼
🔹 Softmax 最大概率的应用意义：
￼1. 预测类别（分类任务）

📌 应用场景：图像分类、文本分类、语音识别等

Softmax 常用于 神经网络的输出层，目的是将模型输出的 raw logits（原始得分）转换为概率分布，并选出概率最大的类别作为最终预测结果。

🔹 示例（图像分类）
假设一个神经网络用于识别图片中的动物，它输出的 logits 是：

z = [2.0, 1.0, 0.1]

通过 Softmax 计算出的概率分布为：

\sigma(z) = [0.659, 0.242, 0.099]

其中最大概率是 0.659（65.9%），对应的类别是 “猫”。因此，我们可以预测 这张图片是猫。

✍ 意义：
	•	通过选择 概率最高的类别，Softmax 让分类任务有清晰的决策依据。
	•	由于 Softmax 归一化后输出概率，可以用于 衡量预测的置信度。

2. 置信度评分

📌 应用场景：自动驾驶、医学诊断、金融风控

Softmax 的概率分布可以用来衡量模型对某个类别的置信度。

🔹 示例（自动驾驶）
假设一个自动驾驶系统检测到前方障碍物，并预测它属于以下类别：

\text{{Softmax 结果}} = [\text{{行人}}: 0.9, \text{{汽车}}: 0.08, \text{{树}}: 0.02]

如果模型 对 “行人” 的预测概率达到 90%，那么系统可以紧急刹车，确保安全。

✍ 意义：
	•	置信度越高，说明模型越确定当前决策。
	•	在医学诊断（如癌症检测）中，医生可以根据 Softmax 概率来决定是否需要进一步检查。

3. 多标签任务的类别权重计算

📌 应用场景：推荐系统、广告投放、情感分析

在某些场景下，我们不只是需要选出一个类别，还要对多个类别进行排序，Softmax 的概率可以作为 类别的权重或重要性。

🔹 示例（电影推荐）
假设一个电影推荐系统预测用户对不同类型电影的兴趣：

\text{{Softmax 结果}} = [\text{{科幻}}: 0.6, \text{{动作}}: 0.3, \text{{爱情}}: 0.1]

	•	系统会 优先推荐科幻电影，其次是动作电影，最后是爱情电影。
	•	而不是只推荐最高概率的电影类型，而是按权重进行推荐。

✍ 意义：
	•	Softmax 让模型能给多个类别分配权重，而不是仅仅做出二元决策。
	•	在广告投放中，可以根据 Softmax 计算的概率，为不同广告分配投放预算。

4. 注意力机制（Transformer 等模型）

📌 应用场景：自然语言处理（NLP），如 ChatGPT、翻译、文本摘要

Softmax 在 Transformer 的 注意力机制（Attention） 中起到了 权重归一化 的作用。

🔹 示例（机器翻译）
在 Transformer 处理文本时，需要关注输入句子中最重要的单词：
	•	输入句子：“I love deep learning.”
	•	计算 Softmax 后，每个单词的注意力权重：

\text{{Softmax 结果}} = [\text{{I}}: 0.1, \text{{love}}: 0.2, \text{{deep}}: 0.3, \text{{learning}}: 0.4]

	•	“learning”（学习） 具有最高注意力权重，因此模型会重点关注这个单词，以便正确翻译或生成文本。

✍ 意义：
	•	让 Transformer 确定哪些单词更重要，提高文本理解能力。
	•	这种权重分配使得 AI 模型在 阅读理解、摘要生成 等任务上表现更好。


